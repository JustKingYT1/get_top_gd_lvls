# scraper/scraper.py
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
from bs4 import BeautifulSoup
import json, re, time, os
from urllib.parse import urljoin
import settings

class DemonlistScraper:
    def __init__(self):
        self.data = []
        self.browser = None
        self.page = None

    def _safe_extract_name(self, text: str) -> str:
        text = text.strip()
        return text.split("-", 1)[1].strip() if "-" in text else text

    def _open_site(self, playwright):
        print("üåê –û—Ç–∫—Ä—ã–≤–∞—é —Å–∞–π—Ç Demonlist.org...")
        self.browser = playwright.chromium.launch(headless=settings.HEADLESS)
        self.page = self.browser.new_page()
        self.page.goto(settings.BASE_URL, wait_until="domcontentloaded", timeout=settings.PAGE_LOAD_TIMEOUT)
        self.page.wait_for_selector('div.w-\\[90\\%\\].mx-auto.grid.justify-items-center', timeout=settings.SELECTOR_TIMEOUT)
        time.sleep(2)

    def _reanimate_scroll(self):
        """'–†–∞—Å–∫–∞—á–∏–≤–∞–µ—Ç' —Å—Ç—Ä–∞–Ω–∏—Ü—É, –µ—Å–ª–∏ –æ–Ω–∞ –ø–µ—Ä–µ—Å—Ç–∞–ª–∞ –ø–æ–¥–≥—Ä—É–∂–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç."""
        print("üò¥ –ü–æ—Ö–æ–∂–µ, –ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ '—É—Å–Ω—É–ª–∞'. –ü—Ä–æ–±—É–µ–º –µ–µ —Ä–∞–∑–±—É–¥–∏—Ç—å...")
        self.page.evaluate("window.scrollBy(0, -500);")
        time.sleep(0.5)
        self.page.evaluate("window.scrollBy(0, document.body.scrollHeight);")
        time.sleep(1)

    def _smart_scroll(self):
        """
        –£–º–Ω—ã–π —Å–∫—Ä–æ–ª–ª, –∫–æ—Ç–æ—Ä—ã–π –∂–¥–µ—Ç –ø–æ–¥–≥—Ä—É–∑–∫–∏ –Ω–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ –±–æ—Ä–µ—Ç—Å—è —Å '–∑–∞—Å—ã–ø–∞–Ω–∏–µ–º' —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
        """
        print("üìú –ù–∞—á–∏–Ω–∞—é —É–º–Ω—ã–π —Å–∫—Ä–æ–ª–ª –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö —É—Ä–æ–≤–Ω–µ–π...")
        prev_count = self.page.evaluate("() => document.querySelectorAll('a[href^=\"/classic/\"]').length")
        no_new_attempts = 0

        while no_new_attempts < settings.MAX_NO_NEW_ATTEMPTS:
            # –î–µ–ª–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –±—ã—Å—Ç—Ä—ã—Ö —Å–∫—Ä–æ–ª–ª–æ–≤
            for _ in range(settings.FAST_SCROLLS_PER_STEP):
                self.page.evaluate("window.scrollBy(0, document.body.scrollHeight)")
                time.sleep(0.3)

            # –¢–µ—Ä–ø–µ–ª–∏–≤–æ –∂–¥–µ–º –ø–æ—è–≤–ª–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            start_time = time.time()
            found_new_in_cycle = False
            while time.time() - start_time < settings.MAX_WAIT_FOR_NEW:
                time.sleep(settings.SCROLL_PAUSE)
                new_count = self.page.evaluate("() => document.querySelectorAll('a[href^=\"/classic/\"]').length")
                if new_count > prev_count:
                    print(f"üîΩ –ù–∞–π–¥–µ–Ω–æ –∫–∞—Ä—Ç–æ—á–µ–∫: {new_count}")
                    prev_count = new_count
                    no_new_attempts = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –Ω–µ—É–¥–∞—á
                    found_new_in_cycle = True
                    break # –í—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞ –æ–∂–∏–¥–∞–Ω–∏—è

            # –ï—Å–ª–∏ –∑–∞ –≤—Å–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ—è–≤–∏–ª–æ—Å—å
            if not found_new_in_cycle:
                no_new_attempts += 1
                print(f"‚è± –ù–µ—Ç –Ω–æ–≤—ã—Ö –∫–∞—Ä—Ç–æ—á–µ–∫ ({no_new_attempts}/{settings.MAX_NO_NEW_ATTEMPTS})")
                self._reanimate_scroll() # –ü—ã—Ç–∞–µ–º—Å—è "—Ä–∞–∑–±—É–¥–∏—Ç—å" —Å—Ç—Ä–∞–Ω–∏—Ü—É
        
        print(f"‚úÖ –°–∫—Ä–æ–ª–ª –∑–∞–≤–µ—Ä—à—ë–Ω. –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {prev_count} –∫–∞—Ä—Ç–æ—á–µ–∫.")


    def _extract_levels_list(self):
        html = self.page.content()
        soup = BeautifulSoup(html, "html.parser")
        cards = soup.select('a[href^="/classic/"]')
        pattern = re.compile(r"/classic/(\d+)")
        for card in cards:
            href = card.get("href", "")
            match = pattern.search(href)
            if not match: continue
            rank = int(match.group(1))
            name_tag = card.select_one("p.font-bold")
            name_raw = name_tag.get_text(strip=True) if name_tag else ""
            self.data.append({"rank": rank, "name": self._safe_extract_name(name_raw), "link": urljoin(settings.BASE_URL, href)})
        self.data.sort(key=lambda x: x["rank"])
        print(f"üß© –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(self.data)} —É—Ä–æ–≤–Ω–µ–π –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞.")

    def _parse_level_details(self, soup: BeautifulSoup) -> dict:
        details = {"length": None, "objects": None, "version": None}
        label_tags = soup.find_all("p", class_="font-bold")
        for tag in label_tags:
            label_text = tag.get_text(strip=True).lower()
            value_tag = tag.find_next_sibling("p")
            if value_tag:
                value_text = value_tag.get_text(strip=True)
                if "length" in label_text: details["length"] = value_text
                elif "objects" in label_text: details["objects"] = int(value_text.replace(",", ""))
                elif "version" in label_text: details["version"] = value_text
        return details

    def _scrape_all_details(self):
        print("\nüîé –ù–∞—á–∏–Ω–∞—é —Å–±–æ—Ä –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –∫–∞–∂–¥–æ–º—É —É—Ä–æ–≤–Ω—é (—ç—Ç–æ –∑–∞–π–º–µ—Ç –≤—Ä–µ–º—è)...")
        for i, level in enumerate(self.data):
            link = level.get("link")
            print(f"[{i+1}/{len(self.data)}] –ó–∞–≥—Ä—É–∂–∞—é: #{level['rank']} {level['name']}")
            try:
                self.page.goto(link, wait_until="domcontentloaded", timeout=settings.PAGE_LOAD_TIMEOUT)
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è —Å–µ–ª–µ–∫—Ç–æ—Ä–∞, —á—Ç–æ–±—ã –¥–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü–µ –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏
                self.page.wait_for_selector('p.font-bold', timeout=15000)
                # –î–∞–µ–º –µ—â–µ –ø–æ–ª—Å–µ–∫—É–Ω–¥—ã –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –µ—Å—Ç—å –∫–∞–∫–∏–µ-—Ç–æ –∞–Ω–∏–º–∞—Ü–∏–∏
                time.sleep(0.5)
                details = self._parse_level_details(BeautifulSoup(self.page.content(), "html.parser"))
                level.update(details)
            except PlaywrightTimeoutError:
                print(f"‚ùå –¢–∞–π–º-–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è —É—Ä–æ–≤–Ω—è #{level['rank']}. –ü—Ä–æ–ø—É—Å–∫–∞—é.")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —É—Ä–æ–≤–Ω—è #{level['rank']}: {e}")

    def _save(self):
        os.makedirs(os.path.dirname(settings.OUTPUT_FILE), exist_ok=True)
        with open(settings.OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)
        print(f"\nüíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {settings.OUTPUT_FILE}")

    def run(self):
        with sync_playwright() as p:
            self._open_site(p)
            self._smart_scroll()
            self._extract_levels_list()
            self._scrape_all_details()
            self.browser.close()
        self._save()